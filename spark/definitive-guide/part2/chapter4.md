## 목차

- 4.0 [구조적 API 개요](#40-구조적-api-개요)
- 4.1 [DataFrame과 Dataset](#41-dataframe과-dataset)
- 4.2 [스키마](#42-스키마)
- 4.3 [스파크의 구조적 데이터 타입 개요(예제)](#43-스파크의-구조적-데이터-타입-개요)
  - 4.3.1 [DataFrame과 Dataset 비교](#431-dataframe과-dataset-비교)
  - 4.3.2 [컬럼](#432-컬럼)
  - 4.3.3 [로우(예제)](#433-로우)
  - 4.3.4 [스파크 데이터 타입(예제)](#434-스파크-데이터-타입)
- 4.4 [구조적 API의 실행 과정](#44-구조적-api의-실행-과정)
  - 4.4.1 [논리적 실행 계획](#441-논리적-실행-계획)
  - 4.4.2 [물리적 실행 계획](#442-물리적-실행-계획)

<br/>

## 4.0 구조적 API 개요

구조적 API는 비정형 로그 파일, 반정형 CSV 파일, 정형적인 파케이(Parquet) 파일까지 다양한 유형의 데이터를 처리할 수 있다.

구조적 API에는 다음과 같은 세 가지 분산 컬렉션 API가 있다.

- **Dataset**

- **DataFrame**

- **SQL 테이블과 뷰**

<br/>

**배치**(**batch**)와 **스트리밍**(**streaming**) 처리에서 구조적 API를 사용할 수 있다. 구조적 API 활용 시

**배치 작업** $\rightarrow$ **스트리밍 작업**, **스트리밍 작업** $\rightarrow$ **배치 작업** 변환이 가능하다.

구조적 API는 데이터 흐름을 정의하는 기본 추상화 개념으로, 이 장에서는 반드시 이해해야 하는 세 가지 기본 개념을 설명한다.

- **타입형(typed) / 비타입형(untyped) API의 개념과 차이점**

- **핵심 용어**

- **스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식**

<br/>

> NOTE\_ 스파크의 기본 개념과 정의를 다시 한번 생각한다. 스파크는 **트랜스포메이션**의 처리 과정을 정의하는 분산 프로그래밍 모델이다.
>
> 사용자가 정의한 다수의 트랜스포메이션은 지향성 비순환 그래프(DAG)로 표현되는 명령을 만들어낸다. 액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행한다.
>
> 트랜스포메이션과 액션으로 다루는 논리적 구조가 DataFrame, Dataset이다.  
> 새로운 DataFrame, Dataset을 생성하려면 트랜스포메이션을 호출해야 한다. 그리고 연산을 시작하거나 사용 언어에 맞는 데이터 타입으로 변환하려면 액션을 호출해야 한다.

<br/>

## 4.1 DataFrame과 Dataset

스파크는 DataFrame과 Dataset이라는 두 가지 구조화된 컬렉션 개념을 가지고 있다.  
둘 사이의 의미적인 차이점을 알아보기 전에 무엇을 나타내는지 먼저 정의한다.

- 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션

  $\rightarrow$ 각 컬럼은 다른 컬럼과 동일한 수의 로우 가짐('값 없음'은 null로 표시)

  $\rightarrow$ 컬렉션의 모든 로우는 같은 데이터 타입 정보 가지고 있음

<br/>

- 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획이며, 불변성 가짐

  $\rightarrow$ DataFrame에 액션 호출 시 스파크는 트랜스포메이션을 실제 실행하고 결과 반환

  $\rightarrow$ 이 과정은 사용자가 원하는 결과를 얻기 위해 로우와 컬럼을 처리하는 방법에 대한 계획을 나타냄

<br/>

기본적으로 테이블과 뷰는 DataFrame과 같다. 대신 테이블은 DataFrame 코드 대신 SQL을 사용한다.

DataFrame과 Dataset을 더 구체적으로 정의하려면 '스키마'를 알아야 한다.  
스키마는 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법이다.

<br/>

## 4.2 스키마

- DataFrame의 컬럼명과 데이터 타입 정의

- 데이터 소스에서 얻거나(스키마 온 리드, schema-on-read) 직접 정의

- 여러 데이터 타입으로 구성

  $\rightarrow$ 어떤 데이터 타입이 어느 위치에 있는지 정의하는 방법 필요

<br/>

## 4.3 스파크의 구조적 데이터 타입 개요

스파크는 사실상 프로그래밍 언어이다. 실행 계획 수립과 처리에 사용하는 **자체 데이터 타입 정보를 가지고 있는 카탈리스트(Catalyst) 엔진**을 사용한다.

카탈리스트 엔진은 다양한 실행 최적화 기능을 제공한다. 스파크는 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블을 가지고 있다.

파이썬, R을 이용해 스파크의 구조적 API를 사용하더라도 대부분의 연산은 파이썬, R의 데이터 타입이 아닌 **스파크의 데이터 타입을 사용**한다.

```scala
val df = spark.range(500).toDF("number")
df.select(df.col("number") + 10)
```

위 예제는 스칼라가 아닌 스파크의 덧셈 연산을 수행한다.

스파크가 지원하는 언어를 이용해 작성된 표현식을 **카탈리스트 엔진에서 스파크의 데이터 타입으로 변환해 명령을 처리**하기 때문이다.

이런 동작이 가능한 이유를 설명하기 전 Dataset을 먼저 알아본다.

<br/>

## 4.3.1 DataFrame과 Dataset 비교

- 본질적으로 구조적 API에는 '비타입형'인 DataFrame과 '타입형'인 Dataset이 존재

  $\rightarrow$ DataFrame을 '비타입형'으로 보는 것이 다소 부정확할 수 있고 DataFrame에도 데이터 타입이 존재,  
  하지만 스키마에 명시된 데이터 타입의 일치 여부를

  **DataFrame** : **런타임**이 되어서 확인

  **Dataset** : **컴파일 타임**에 확인

<br/>

- Dataset은 JVM 기반의 언어인 스칼라, 자바에서만 지원

  $\rightarrow$ Dataset의 데이터 타입 정의하려면 스칼라의 case class, JavaBean을 사용해야 한다.

  $\rightarrow$ 파이썬, R에서 사용 불가능하나 최적화된 포맷인 DataFrame으로 처리 가능하다.

<br/>

- **DataFrame** : **Row 타입으로 구성된 Dataset**

  $\rightarrow$ Row 타입은 스파크가 사용하는 '연산에 최적화된 인메모리 포맷'의 내부적인 표현 방식

  $\rightarrow$ Row 타입을 사용하면 자체 데이터 포맷 사용해 매우 효율적 연산 가능하다.  
  (가비지 컬렉션(garbage collection)과 객체 초기화 부하가 있는 JVM 데이터 타입 사용 대신)

<br/>

기억해야 할 것은 **DataFrame을 사용하면 스파크의 최적화된 내부 포맷을 사용**할 수 있다는 것이다.

스파크의 최적화된 내부 포맷을 사용하면 스파크가 지원하는 어떤 언어 API를 사용하더라도 동일한 효과와 효율성 얻을 수 있다.

<br/>

## 4.3.2 컬럼

- 정수형이나 문자열 같은 **단순 데이터 타입**

- 배열이나 맵 같은 **복합 데이터 타입**

- **null 값**

  을 표현한다.

<br/>

스파크는 데이터 타입의 모든 정보를 추적하며 다양한 컬럼 변환 방법을 제공한다.  
스파크의 컬럼은 테이블의 컬럼으로 생각할 수 있다.

<br/>

## 4.3.3 로우

- 로우 $\rightarrow$ 데이터 레코드

- DataFrame의 레코드는 Row 타입으로 구성

- SQL, RDD, 데이터 소스에서 얻거나 직접 생성

<br/>

아래 예제는 range 메서드를 사용해 DataFrame을 생성한다.

```scala
spark.range(2).toDF().collect()
```

$\rightarrow$ 스칼라 코드

```python
spark.range(2).collect()
```

$\rightarrow$ 파이썬 코드

두 코드 모두 Row 객체로 이루어진 배열을 반환한다.

<img width="350" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/8c2a6cfb-931c-4c3f-bbf7-e8ed4b7ea62a">

<br/>

## 4.3.4 스파크 데이터 타입

스파크는 여러 가지 내부 데이터 타입을 가지고 있다.  
특정 데이터 타입의 컬럼을 초기화하고 정의하는 방법을 알아본다.

스파크 데이터 타입을 스칼라에서 사용하려면 아래와 같은 코드를 사용한다.

```scala
import org.apache.spark.sql.types._

val b = ByteType
```

<br/>

스파크 데이터 타입을 자바에서 사용하려면 다음 패키지의 팩토리 메서드를 사용한다.

```java
import org.apache.spark.sql.types.DataTypes;

ByteType x = DataTypes.ByteType;
```

<br/>

스파크 데이터 타입을 파이썬에서 사용하려면 아래와 같은 코드를 사용한다.

```python
from pyspark.sql.types import *

b = ByteType()
```

[스파크 공식 문서(언어별 데이터 타입 매핑)](https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.html)에서 다양한 프로그래밍 언어의 데이터 타입이 스파크의 어떤 데이터 타입과 매핑되는지 확인할 수 있다.

고정형 DataFrame을 그대로 사용하는 경우는 거의 없으며, 대부분 DataFrame의 처리와 변환을 수행한다. 따라서 구조적 API의 실행 과정을 알아야 한다.

<br/>

## 4.4 구조적 API의 실행 과정

스파크 코드가 클러스터에서 실제 처리되는 과정을 설명한다.

구조적 API 쿼리가 사용자 코드에서 실제 실행 코드로 변환되는 진행 과정이다.

1. DataFrame / Dataset / SQL을 이용해 코드를 작성

2. 정상적인 코드라면 스파크가 **논리적 실행 계획**으로 변환

3. 스파크는 **논리적 실행 계획**을 **물리적 실행 계획**으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인

4. 스파크는 클러스터에서 **물리적 실행 계획**(**RDD 처리**)을 실행

<br/>

실행할 코드를 작성해야 한다. 작성한 스파크 코드는 콘솔이나 **spark-submit** 셸 스크립트로 실행한다.  
**카탈리스트 옵티마이저**(**Catalyst Optimizer**)는 코드를 넘겨받고 실제 실행 계획을 생성한다.  
마지막으로 스파크는 코드를 실행한 후 결과를 반환한다.

아래 그림에서 전체적인 과정을 확인할 수 있다.

<img width="400" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/8d3a0735-8ce4-42ed-bb18-55c936802f6e">

<br/>

## 4.4.1 논리적 실행 계획

첫 번째 실행 단계에서는 사용자 코드를 논리적 실행 계획으로 변환한다.

아래 그림에서 변환 과정을 확인할 수 있다.

<img width="500" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/f0de0c23-fcd5-47fc-8f25-847befaf5127">

$\rightarrow$ 구조적 API의 논리적 실행 계획 수립 과정

<br/>

- 논리적 실행 계획 단계에서는 추상적 트랜스포메이션만 표현

- 드라이버, 익스큐터의 정보 고려 X

- 사용자의 다양한 표현식을 최적화된 버전으로 변환

- 사용자 코드 $\rightarrow$ **검증 전 논리적 실행 계획**(**unresolved logical plan**)으로 변환

- 코드의 유효성과 테이블, 컬럼의 존재 여부만을 판단하는 과정, 아직 실행 계획을 검증하지 않은 상태

<br/>

스파크 **분석기**(**analyzer**)는 컬럼과 테이블을 **검증**하기 위해 **카탈로그**, 모든 테이블의 저장소, DataFrame 정보를 활용한다.  
필요한 테이블, 컬럼이 카탈로그에 없다면 검증 전 논리적 실행 계획이 만들어지지 않는다.

테이블과 컬럼에 대한 **검증 결과는 카탈리스트 옵티마이저로 전달**된다.

**카탈리스트 옵티마이저** $\rightarrow$ 조건절 푸시 다운(predicate pushing down)이나 선택절 구문을 이용해 논리적 실행 계획을 최적화하는 규칙의 모음

필요한 경우 도메인에 최적화된 규칙을 적용할 수 있는 카탈리스트 옵티마이저의 확장형 패키지를 만들 수 있다.

<br/>

## 4.4.2 물리적 실행 계획

이어서 물리적 실행 계획을 생성하는 과정이 시작된다.

스파크 실행 계획이라고도 불리는 **물리적 실행 계획**은 **논리적 실행 계획을 클러스터 환경에서 실행하는 방법**을 정의한다.

<img width="550" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/4016372f-2b9b-4c31-a04f-36abfb30b94a">

$\rightarrow$ 물리적 실행 계획 수립 과정

**다양한 물리적 실행 전략을 생성**하고 비용 모델을 이용해 비교한 후 **최적의 전략을 선택**한다.

비용 비교 예시 $\rightarrow$ 테이블 크기, 파티션 수 등의 물리적 속성을 고려해 지정된 조인 연산 수행에 필요한 비용을 계산하고 비교

물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환된다.  
스파크는 DataFrame, Dataset, SQL로 정의된 쿼리를 RDD 트랜스포메이션으로 컴파일한다. 따라서 스파크를 '**컴파일러**'라고 부르기도 한다.
