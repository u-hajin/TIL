## 목차

- 1.1 [아파치 스파크의 철학](#11-아파치-스파크의-철학)
- 1.2 [스파크의 등장 배경](#12-스파크의-등장-배경)
- 1.3 [스파크의 역사](#13-스파크의-역사)
- 1.5 [스파크 실행하기](#15-스파크-실행하기)
  - 1.5.1 [로컬 환경에 스파크 내려받기](#151-로컬-환경에-스파크-내려받기)
  - 1.5.2 [스파크 대화형 콘솔 실행하기](#152-스파크-대화형-콘솔-실행하기)
    - [파이썬 콘솔 실행하기](#파이썬-콘솔-실행하기)
    - [SQL 콘솔 실행하기](#sql-콘솔-실행하기)

## 1.1 아파치 스파크의 철학

'**빅데이터를 위한 통합 컴퓨팅 엔진과 라이브러리 집합**'이라는 문장으로 아파치 스파크를 설명한다.

### 통합

스파크의 핵심 목표 :

> '빅데이터 애플리케이션 개발에 필요한 통합 플랫폼을 제공하자'

스파크는 간단한 데이터 읽기, SQL 처리, 머신러닝, 스트림 처리까지 다양한 데이터 분석 작업을 같은 연산 엔진과 일관성 있는 API로 수행할 수 있도록 설계되어 있다.

스파크의 개발 사상은 현실 세계의 데이터 분석 작업이 다양한 처리 유형과 라이브러리를 결합해 수행된다는 통찰에서 비롯되었다.

스파크의 통합 특성을 이용하면 기존의 데이터 분석 작업을 더 쉽고 효율적으로 수행할 수 있다.

- 일관성 있는 조합형 API 제공

  $\rightarrow$ 작은 코드 조각이나 기존 라이브러리를 사용해 애플리케이션을 만들 수 있다.

- 직접 스파크 기반의 라이브러리를 제작

  $\rightarrow$ 스파크의 API는 사용자 애플리케이션에서 다른 라이브러리의 기능을 조합해 더 나은 성능을 발휘할 수 있도록 설계되었다.

  e.g. SQL 쿼리로 데이터를 읽고 ML 라이브러리로 머신러닝 모델을 평가할 경우 $\rightarrow$ 스파크 엔진이 두 단계를 하나로 병합, 데이터를 한번만 조회할 수 있도록 한다.

### 컴퓨팅 엔진

스파크는 '통합'이라는 관점을 중시하며 기능의 범위를 컴퓨팅 엔진으로 제한 $\rightarrow$ 저장소 시스템의 데이터를 연산하는 역할만 수행할 뿐 영구 저장소 역할은 수행하지 않는다.

- Azure Storage
- Amazon S3
- Apache Hadoop
- Apache Cassandra
- Apache Kafka

그 대신 위와 같은 저장소들을 지원한다.

- 대부분의 데이터는 여러 저장소 시스템에 혼재되어 저장  
  $\rightarrow$ 스파크는 내부에 데이터를 오랜 시간 저장하지 않으며, 특정 저장소 시스템을 선호하지 않는다.

- 데이터 이동은 높은 비용을 유발  
  $\rightarrow$ 스파크는 데이터 저장 위치에 상관없이 처리에 집중하도록 만들어졌다.

또한 사용자 API는 서로 다른 저장소 시스템을 매우 유사하게 볼 수 있도록 만들어졌기에 애플리케이션은 데이터가 저장된 위치를 신경 쓸 필요가 없다.

**연산 기능**에 초점을 맞추면서 기존 빅데이터 플랫폼과 차별화하고 있다.

### 라이브러리

스파크는 엔진에서 제공하는 표준 라이브러리와 오픈 소스 커뮤니티에서 서드파티 패키지 형태로 제공하는 다양한 외부 라이브러리를 지원한다.

사실 스파크의 표준 라이브러리는 여러 오픈 소스 프로젝트의 집합체이다.

스파크 코어 엔진 자체는 큰 변화가 없었지만 라이브러리는 더 많은 기능을 제공하기 위해 꾸준히 변해왔다.

- 스파크 SQL : SQL과 구조화된 데이터 제공
- MLlib : 머신러닝 지원
- 스파크 스트리밍, 구조적 스트리밍 : 스트림 처리 기능 제공
- GraphX 라이브러리 : 그래프 분석 엔진

외부 라이브러리 목록은 [Third-Party Packages for Apache Spark](https://spark-packages.org)에서 확인할 수 있다.

## 1.2 스파크의 등장 배경

컴퓨터 애플리케이션과 하드웨어의 바탕을 이루는 경제적 요인의 변화 $\rightarrow$ 데이터 분석에 새로운 처리 엔진과 프로그래밍 모델이 필요해짐

프로세서의 성능 향상 때문에 애플리케이션은 코드 수정 없이도 빨라졌다. 대규모 애플리케이션은 이런 경향에 맞춰 만들어졌고 더 많은 연산과 대규모 데이터 처리를 프로세서의 성능 향상에 맡겼다.  
하지만 하드웨어의 성능 향상은 2005년에 멈췄으며 병렬 CPU 코어를 더 많이 추가하는 방향으로 선회했다.

계속해서 데이터 수집 비용은 저렴해졌지만, 데이터는 클러스터에서 처리해야 할 만큼 거대해졌다.  
또한 데이터 처리 애플리케이션에 적용한 전통적인 프로그래밍 모델도 더는 힘을 발휘하지 못해 새로운 프로그래밍 모델이 필요해졌다. $\rightarrow$ 아파치 스파크 탄생

## 1.3 스파크의 역사

UC버클리 대학교에서 2009년 스파크 연구 프로젝트로 시작했다.

당시 하둡 맵리듀스는 수천 개의 노드로 구성된 클러스터에서 병렬로 데이터를 처리하는 최초의 오픈 소스 시스템이자 클러스터 환경용 병렬 프로그래밍 엔진의 대표주자였다.

UC버클리 대학교의 하둡 사용자들과 플랫폼의 요건을 연구한 결과 아래 2가지 사실이 명확해졌다.

1. 클러스터 컴퓨팅이 엄청난 잠재력을 가지고 있다.

   맵리듀스에 경험이 있는 조직은 자체 데이터를 활용해 기존과는 다른 애플리케이션을 만들어냈다.

2. 맵리듀스 엔진을 사용하는 대규모 애플리케이션의 난이도와 효율성 문제

   전통적인 머신러닝 알고리즘 $\rightarrow$ 데이터를 10~20회 가량 처리

   맵리듀스로 처리 $\rightarrow$ 단계별로 맵리듀스 잡을 개발, 클러스터에서 각각 실행해야 해 매번 데이터를 처음부터 읽어야 함.

위 문제를 해결하기 위해

- 함수형 프로그래밍 기반의 API 설계

  $\rightarrow$ 여러 단계로 이루어진 애플리케이션을 간결하게 개발할 수 있다.

- 새로운 엔진 기반의 API 구현

  $\rightarrow$ 연산 단계 사이에서 메모리에 저장된 데이터를 효율적으로 공유할 수 있다.

스파크가 제공하는 조합형 API 핵심 아이디어도 진화했다.

- 1.0 이전 스파크 초기 버전

  $\rightarrow$ **함수형 연산**(자바 객체로 이루어진 컬렉션에 맵이나 리듀스 같은 병렬 연산 수행 방식) 관점에서 API 정의

- 1.0 버전

  $\rightarrow$ 구조화된 데이터를 기반으로 동작하는 신규 API인 **스파크 SQL** 추가

  스파크 SQL은 데이터 포맷과 코드를 잘 이해하는 라이브러리와 API를 이용해 새롭고 강력한 최적화 기능을 제공

- 그 이후

  $\rightarrow$ DataFrame, 머신러닝 파이프라인, 자동 최적화 수행 구조적 스트리밍 등 더 강력한 **구조체 기반의 신규 API** 추가

## 1.5 스파크 실행하기

스파크는 스칼라로 구현되어 자바 가상 머신 기반으로 동작한다. 책에서 제공하는 스파크 관련 코드는 대부분 대화형 실행 방식으로 연습할 수 있다.

## 1.5.1 로컬 환경에 스파크 내려받기

[스파크 프로젝트 공식 홈페이지](https://spark.apache.org/downloads.html)에서 패키지 유형을 'Pre-built for Hadoop 2.7 and later'로 선택 후 내려받은 파일의 압축을 해제한다.

## 1.5.2 스파크 대화형 콘솔 실행하기

### 파이썬 콘솔 실행하기

파이썬 콘솔 실행을 위해 3.x 버전이 설치되어 있어야 한다.
스파크 홈 디렉토리에서 아래 명령어를 사용해 파이썬 콘솔을 실행한다.

```bash
./bin/pyspark
```

<img width="500" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/f44a3670-18c7-44f6-9988-025be68e0537">

콘솔 실행 후 spark를 입력하면 SparkSession 객체가 출력된다.

<img width="500" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/cae00d30-3a81-437f-a6e2-c0db5ee59ff3">

### 스칼라 콘솔 실행하기

스칼라 콘솔 실행을 위해 아래 명령어를 실행한다.

```bash
./bin/spark-shell
```

<img width="500" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/e682c539-11d4-40fe-95db-7f7bee937044">

콘솔 실행 후 spark를 입력하면 SparkSession 객체가 출력된다.

### SQL 콘솔 실행하기

스파크 SQL을 사용할 수 있는 SQL 콘솔은 아래 명령어로 실행한다.

```bash
./bin/spark-sql
```

<img width="500" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/4bbd8fca-27fa-4dba-b49c-c08600dbe427">

스파크 SQL은 2부에서 자세히 다룬다.
