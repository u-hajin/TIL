Spark: The Definitive Guide(스파크 완벽 가이드) 책을 읽고 정리합니다.

## 목차

1.1 [아파치 스파크의 철학](#11-아파치-스파크의-철학)

## 1.1 아파치 스파크의 철학

'**빅데이터를 위한 통합 컴퓨팅 엔진과 라이브러리 집합**'이라는 문장으로 아파치 스파크를 설명한다.

### 통합

스파크의 핵심 목표 :

> '빅데이터 애플리케이션 개발에 필요한 통합 플랫폼을 제공하자'

스파크는 간단한 데이터 읽기, SQL 처리, 머신러닝, 스트림 처리까지 다양한 데이터 분석 작업을 같은 연산 엔진과 일관성 있는 API로 수행할 수 있도록 설계되어 있다.

스파크의 개발 사상은 현실 세계의 데이터 분석 작업이 다양한 처리 유형과 라이브러리를 결합해 수행된다는 통찰에서 비롯되었다.

스파크의 통합 특성을 이용하면 기존의 데이터 분석 작업을 더 쉽고 효율적으로 수행할 수 있다.

- 일관성 있는 조합형 API 제공

  $\rightarrow$ 작은 코드 조각이나 기존 라이브러리를 사용해 애플리케이션을 만들 수 있다.

- 직접 스파크 기반의 라이브러리를 제작

  $\rightarrow$ 스파크의 API는 사용자 애플리케이션에서 다른 라이브러리의 기능을 조합해 더 나은 성능을 발휘할 수 있도록 설계되었다.

  e.g. SQL 쿼리로 데이터를 읽고 ML 라이브러리로 머신러닝 모델을 평가할 경우 $\rightarrow$ 스파크 엔진이 두 단계를 하나로 병합, 데이터를 한번만 조회할 수 있도록 한다.

### 컴퓨팅 엔진

스파크는 '통합'이라는 관점을 중시하며 기능의 범위를 컴퓨팅 엔진으로 제한 $\rightarrow$ 저장소 시스템의 데이터를 연산하는 역할만 수행할 뿐 영구 저장소 역할은 수행하지 않는다.

- Azure Storage
- Amazon S3
- Apache Hadoop
- Apache Cassandra
- Apache Kafka

그 대신 위와 같은 저장소들을 지원한다.

- 대부분의 데이터는 여러 저장소 시스템에 혼재되어 저장  
  $\rightarrow$ 스파크는 내부에 데이터를 오랜 시간 저장하지 않으며, 특정 저장소 시스템을 선호하지 않는다.

- 데이터 이동은 높은 비용을 유발  
  $\rightarrow$ 스파크는 데이터 저장 위치에 상관없이 처리에 집중하도록 만들어졌다.

또한 사용자 API는 서로 다른 저장소 시스템을 매우 유사하게 볼 수 있도록 만들어졌기에 애플리케이션은 데이터가 저장된 위치를 신경 쓸 필요가 없다.

**연산 기능**에 초점을 맞추면서 기존 빅데이터 플랫폼과 차별화하고 있다.

### 라이브러리

스파크는 엔진에서 제공하는 표준 라이브러리와 오픈 소스 커뮤니티에서 서드파티 패키지 형태로 제공하는 다양한 외부 라이브러리를 지원한다.

사실 스파크의 표준 라이브러리는 여러 오픈 소스 프로젝트의 집합체이다.

스파크 코어 엔진 자체는 큰 변화가 없었지만 라이브러리는 더 많은 기능을 제공하기 위해 꾸준히 변해왔다.

- 스파크 SQL : SQL과 구조화된 데이터 제공
- MLlib : 머신러닝 지원
- 스파크 스트리밍, 구조적 스트리밍 : 스트림 처리 기능 제공
- GraphX 라이브러리 : 그래프 분석 엔진

외부 라이브러리 목록은 [Third-Party Packages for Apache Spark](https://spark-packages.org)에서 확인할 수 있다.
