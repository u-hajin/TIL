## 목차

- 2.1 [스파크의 기본 아키텍처](#21-스파크의-기본-아키텍처)
  - 2.1.1 [스파크 애플리케이션](#211-스파크-애플리케이션)
    - [Driver](#driver)
    - [Executor](#executor)
    - [스파크 애플리케이션 이해를 위한 핵심사항](#스파크-애플리케이션-이해를-위한-핵심사항)
- 2.2 [스파크의 다양한 언어 API](#22-스파크의-다양한-언어-api)
  - [SparkSession과 스파크 언어 API 간의 관계](#sparksession과-스파크-언어-api-간의-관계)
- 2.3 [스파크 API](#23-스파크-api)
- 2.4 [스파크 시작하기](#24-스파크-시작하기)
- 2.5 [SparkSession](#25-sparksession)
  - [일정 범위 숫자 생성(예제)](#일정-범위-숫자-생성)
- 2.6 [DataFrame](#26-dataframe)
  - 2.6.1 [파티션](#261-파티션)
- 2.7 [트랜스포메이션(예제)](#27-트랜스포메이션)

## 2.1 스파크의 기본 아키텍처

컴퓨터 클러스터는 여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있게 만든다.  
하지만 컴퓨터 클러스터를 구성하는 것만으로는 부족하며 클러스터에서 작업을 조율할 수 있는 프레임워크인 스파크가 필요하다.

**스파크는 클러스터의 데이터 처리 작업을 관리하고 조율**한다.

스파크가 연산에 사용할 클러스터는

- 스파크 standalone
- 하둡 YARN
- Mesos

위와 같은 클러스터 매니저에서 관리한다.

### 작업 처리 과정

1. 클러스터 매니저에 스파크 애플리케이션 **submit**

2. 클러스터 매니저는 애플리케이션 실행에 필요한 **자원 할당**

3. 할당받은 자원으로 **작업 처리**

## 2.1.1 스파크 애플리케이션

스파크 애플리케이션 $\rightarrow$ dirver(드라이버) 프로세스 + 다수의 executor(익스큐터) 프로세스로 구성

### Driver

- 클러스터 노드 중 하나에서 실행됨

- main() 함수 실행

- 스파크 애플리케이션 정보 유지 관리

- 사용자 프로그램이나 입력에 대한 응답

- 전박적인 익스큐터 프로세스의 작업 관련 분석 및 배포

- 스케줄링

스파크 애플리케이션의 심장과 같은 존재로 애플리케이션의 수명 주기 동안 관련 정보를 모두 유지한다.

### Executor

- 드라이버 프로세스가 할당한 작업(코드) 실행

- 진행 상황을 드라이버 노드에 보고

아래 이미지는 클러스터 매니저가 물리적 머신을 관리하고 스파크 애플리케이션에 자원을 할당하는 방법을 나타낸다.

<img width="350" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/4420468f-de96-4941-a2ea-51327438045a">

$\rightarrow$ 스파크 애플리케이션의 아키텍처

클러스터 매니저는

- 스파크 standalone
- 하둡 YARN
- Mesos

중 하나를 선택할 수 있다.

하나의 클러스터에서 여러개의 스파크 애플리케이션을 실행할 수 있다.

위 이미지에서 왼쪽에 드라이버 프로세스가 있고 오른쪽에 4개의 익스큐터가 있다. 사용자는 각 노드에 할당할 익스큐터 수를 지정할 수 있다. (이미지에는 클러스터 노드의 개념을 나타내지 않음.)

### 스파크 애플리케이션 이해를 위한 핵심사항

- 스파크는 사용 가능한 자원을 파악하기 위해 클러스터 매니저 사용

- 드라이버 프로세스는 주어진 작업 완료를 위해 드라이버 프로그램의 명령을 익스큐터에서 실행할 책임이 있다.

익스큐터는 대부분 스파크 코드를 실행하는 역할을 한다. 하지만 드라이버는 스파크의 언어 API를 통해 다양한 언어로 실행할 수 있다.

## 2.2 스파크의 다양한 언어 API

스파크의 언어 API를 이용해 다양한 프로그래밍 언어로 스파크 코드를 실행할 수 있다.  
스파크는 모든 언어에 맞는 '핵심 개념'을 제공한다. 이러한 핵심 개념은 클러스터 머신에서 실행되는 스파크 코드로 변환된다.

구조적 API만으로 작성된 코드는 언어에 상관없이 유사한 성능을 발휘한다.

- **스칼라**

  $\rightarrow$ 스파크는 스칼라로 개발되어 있으므로 스칼라가 스파크의 기본 언어이다. 책에서는 스칼라 예제를 대부분 제공한다.

- **자바**

  $\rightarrow$ 스파크 창시자들은 자바를 이용해 스파크 코드를 작성할 수 있도록 했다.

- **파이썬**

  $\rightarrow$ 스칼라가 지원하는 거의 모든 구조를 지원한다.

- **SQL**

  $\rightarrow$ 스파크는 ANSI SQL:2003 표준 중 일부를 지원한다.

- **R**

  $\rightarrow$ 스파크에는 일반적으로 사용하는 2개의 R 라이브러리가 존재한다. 하나는 스파크 코어에 포함된 SparkR이고, 다른 하나는 R 커뮤니티 기반 패키지인 sparklyr이다.

### SparkSession과 스파크 언어 API 간의 관계

<img width="450" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/7d903c44-b864-4aaa-9772-4779742ae0e7">

$\rightarrow$ SparkSession과 스파크 언어 API 간의 관계

사용자는 스파크 코드를 실행하기 위해 SparkSession 객체를 진입점으로 사용할 수 있다.

스파크가 파이썬이나 R로 작성한 코드를 익스큐터의 JVM에서 실행할 수 있는 코드로 변환 $\rightarrow$ 사용자는 JVM 코드를 명시적으로 작성하지 않는다.

## 2.3 스파크 API

**저수준의 비구조적(unstructured) API**, **고수준의 구조적(structured) API**를 스파크가 기본적으로 제공 $\rightarrow$ 다양한 언어로 스파크 사용 가능

## 2.4 스파크 시작하기

실제 스파크 애플리케이션을 개발하려면 **사용자 명령**과 **데이터**를 스파크 애플리케이션에 **전송**하는 방법을 알아야 한다.

대화형 모드로 스파크를 시작하면 스파크 애플리케이션을 관리하는 SparkSession이 자동으로 생성된다.

하지만 스탠드얼론 애플리케이션으로 스파크를 시작하려면 사용자 애플리케이션 콛에서 SparkSession 객체를 직접 생성해야 한다.

예제 실행을 위해 [스칼라 콘솔 실행](https://github.com/usuyn/TIL/blob/master/spark/definitive-guide/part1/chapter1.md#스칼라-콘솔-실행하기)을 참고해 대화형 세션을 시작한다.

## 2.5 SparkSession

- 스파크 애플리케이션은 SparkSession이라 불리는 드라이버 프로세스로 제어

- SparkSession 인스턴스는 사용자가 정의한 처리 명령을 클러스터에서 실행

- 하나의 SparkSession은 하나의 스파크 애플리케이션에 대응

스칼라와 파이썬 콘솔을 실행하면 spark 변수로 SparkSession을 사용할 수 있다.

<img width="500" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/3f3d521e-9a0e-4d07-bfa4-e736a77fb339">

$\rightarrow$ 스칼라에서 SparkSession 확인

### 일정 범위 숫자 생성

일정 범위의 숫자를 만드는 간단한 작업을 수행한다. 이 숫자들은 스프레드시트에서 컬럼명을 지정한 것과 같다.

```scala
val myRange = spark.range(1000).toDF("number")
```

$\rightarrow$ 스칼라 코드

```python
myRange = spark.range(1000).toDF("number")
```

$\rightarrow$ 파이썬 코드

<img width="400" height="auto" src="https://github.com/usuyn/TIL/assets/68963707/693e7a34-875d-4223-8466-c760d23e6acd">

생성한 DataFrame은 한 개의 컬럼(number)과 1,000개의 로우로 구성되며 각 로우에 0부터 999까지의 값이 할당되어 있다.

이 숫자들은 **분산 컬렉션**을 나타낸다.

클러스터 모드에서 코드 예제를 실행하면 숫자 범위의 각 부분이 서로 다른 익스큐터에 할당된다.

## 2.6 DataFrame

- 가장 대표적인 **구조적 API**

- 테이블의 데이터를 로우와 컬럼으로 단순하게 표현

- 컬럼과 컬럼의 타입을 정의한 목록 $\rightarrow$ **스키마(schema)**

컬럼에 이름을 붙인 스프레드시트와 비슷하나 아래와 같은 차이점이 존재한다.

<img width="400" alt="image" src="https://github.com/usuyn/TIL/assets/68963707/ee17fb23-081f-41b0-aa7e-61502b9676ac">

$\rightarrow$ 분산 컴퓨터와 단일 컴퓨터 분석의 차이점

스프레드시트는 한 대의 컴퓨터에 있지만, 스파크 DataFrame은 수천 대의 컴퓨터에 분산되어 있다.

단일 컴퓨터에 저장하기에는 데이터가 너무 크거나 계산에 오랜 시간 소요 $\rightarrow$ 여러 컴퓨터에 데이터를 분산

DataFrame은 스파크에서만 사용하는 개념이 아니다. 파이썬과 R 모두 비슷한 개념을 가지고 있으나 일반적(예외사항 존재)으로 분산 컴퓨터가 아닌 단일 컴퓨터에 존재한다.  
이러한 상황에서는 DataFrame으로 수행할 수 있는 작업이 해당 머신이 가진 자원에 따라 제한된다.

스파크는 파이썬과 R 언어를 모두 지원한다. 따라서

파이썬 Pandas 라이브러리 DataFrame, R의 DataFrame $\rightarrow$ 스파크 DataFrame으로 쉽게 변환 가능

스파크는 분산 데이터 모음을 표현하는 Dataset, DataFrame, SQL 테이블, RDD라는 핵심 추상화 개념을 가지고 있다. 이중 DataFrame은 가장 쉽고 효율적이다.

## 2.6.1 파티션

스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션(청크 단위)으로 데이터를 분할한다.  
파티션은 클러스터의 물리적 머신에 존재하는 로우의 집합을 의미한다.

DataFrame의 파티션은 실행 중에 데이터가 컴퓨터 클러스터에서 물리적으로 분산되는 방식을 나타낸다.

- 파티션이 하나라면 수천 개의 익스큐터가 있더라도 병렬성 1

- 수백 개의 파티션이 있더라도 익스큐터가 하나밖에 없다면 병렬성 1

물리적 파티션에 데이터 변환용 함수를 지정하면 스파크가 실제 처리 방법 결정 $\rightarrow$ DataFrame 사용 시 파티션을 수동, 개별적으로 처리할 필요 없음

## 2.7 트랜스포메이션

스파크의 핵심 데이터 구조는 불변성(immutable)을 가진다. 즉, 한번 생성하면 변경할 수 없다.

DataFrame을 변경하려면 **트랜스포메이션**이라 불리는 명령을 사용해 원하는 변경 방법을 스파크에 알려줘야 한다.

아래 코드는 DataFrame에서 짝수를 찾는 간단한 트랜스포메이션 예제이다.

```scala
val divisBy2 = myRange.where("number % 2 = 0")
```

$\rightarrow$ 스칼라 코드

```python
divisBy2 = myRange.where("number % 2 = 0")
```

$\rightarrow$ 파이썬 코드

위 코드를 실행하면 응답값이 출력되지만 결과는 출력되지 않는다.  
추상적인 트랜스포메이션만 지정한 상태이기 때문에 액션(action)을 호출하지 않으면 스파크는 실제 트랜스포메이션을 수행하지 않는다.

트랜스포메이션은 스파크에서 비즈니스 로직을 표현하는 핵심 개념이다.  
두 가지 유형이 존재한다.

- Narrow Dependency, 좁은 의존성

- Wide Dependency, 넓은 의존성
